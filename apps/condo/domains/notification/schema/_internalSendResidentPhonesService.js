/**
 * Generated by `createservice notification._internalSendResidentPhonesService '--type=mutations' '--force=true'`
 */

const fs = require('fs')

const { stringify } = require('csv-stringify')
const dayjs = require('dayjs')
const Upload = require('graphql-upload/Upload.js')
const { get, map, isFunction } = require('lodash')

const { GQLError, GQLErrorCode: { BAD_USER_INPUT, INTERNAL_ERROR } } = require('@open-condo/keystone/errors')
const { GQLCustomSchema, itemsQuery } = require('@open-condo/keystone/schema')

const { NOT_FOUND } = require('@condo/domains/common/constants/errors')
const { md5 } = require('@condo/domains/common/utils/crypto')
const { loadListByChunks } = require('@condo/domains/common/utils/serverSchema')
const { getTmpFile } = require('@condo/domains/common/utils/testSchema/file')
const access = require('@condo/domains/notification/access/_internalSendResidentPhonesService')
const { RESIDENT } = require('@condo/domains/user/constants/common')

const { Contact } = require('../../contact/utils/serverSchema')
const { User } = require('../../user/utils/serverSchema')


const CHUNK_SIZE = 1000

const buildUploadInputFrom = ({ stream, filename, mimetype, encoding, meta }) => {
    const uploadData = {
        createReadStream: () => {
            return stream
        },
        filename,
        mimetype,
        encoding,
        meta,
    }
    const uploadInput = new Upload()
    uploadInput.promise = new Promise(resolve => {
        resolve(uploadData)
    })
    return uploadInput
}

const queryAllItemsByChunks = async ({
    schemaName,
    where = {},
    chunkSize = 100,
    chunkProcessor = (chunk) => chunk,
}) => {
    let skip = 0
    let newChunk = []
    let all = []
    let newChunkLength

    do {
        newChunk = await itemsQuery(schemaName, { where, first: chunkSize, skip, sortBy: ['id_ASC'] })
        newChunkLength = newChunk.length

        if (newChunkLength > 0) {
            if (isFunction(chunkProcessor)) {
                newChunk = chunkProcessor.constructor.name === 'AsyncFunction'
                    ? await chunkProcessor(newChunk)
                    : chunkProcessor(newChunk)
            }

            skip += newChunkLength
            all = all.concat(newChunk)
        }
    } while (newChunkLength)

    return all
}


const _internalSendResidentPhonesService = new GQLCustomSchema('_internalSendResidentPhonesService', {
    types: [
        {
            access: true,
            type: 'input _internalSendResidentPhonesInput { dv: Int!, sender: SenderFieldInput! }',
        },
        {
            access: true,
            type: 'type _internalSendResidentPhonesOutput { ok: Boolean! }',
        },
    ],
    
    mutations: [
        {
            access: access.can_internalSendResidentPhones,
            schema: '_internalSendResidentPhones(data: _internalSendResidentPhonesInput!): _internalSendResidentPhonesOutput',
            resolver: async (parent, args, context, info, extra = {}) => {
                const filename = getTmpFile('csv')
                const writeStream = fs.createWriteStream(filename, { encoding: 'utf8' })

                const stringifier = stringify({ header: true, columns: ['phone'] })
                stringifier.pipe(writeStream)

                for (let i = 0; i < 7000; i++) {
                    await queryAllItemsByChunks({
                        schemaName: 'Contact',
                        where: { phone_not: null, deletedAt: null },
                        chunkSize: CHUNK_SIZE,
                        chunkProcessor: async (chunk) => {
                            const phones = map(chunk, 'phone')

                            for (const phone of phones) {
                                const formattedPhone = phone.slice(1)

                                if (/^7\d{10}$/.test(formattedPhone)) {
                                    stringifier.write([md5(formattedPhone)])
                                }
                            }

                            return []
                        },
                    })

                    await queryAllItemsByChunks({
                        schemaName: 'User',
                        where: { type: RESIDENT, phone_not: null, deletedAt: null },
                        chunkSize: CHUNK_SIZE,
                        chunkProcessor: async (chunk) => {
                            const phones = map(chunk, 'phone')

                            for (const phone of phones) {
                                const formattedPhone = phone.slice(1)

                                if (/^7\d{10}$/.test(formattedPhone)) {
                                    stringifier.write([md5(formattedPhone)])
                                }
                            }

                            return []
                        },
                    })
                }

                stringifier.end()

                await new Promise((resolve, reject) => {
                    writeStream.on('finish', resolve)
                    writeStream.on('error', reject)
                })

                const stream = fs.createReadStream(filename, { encoding: 'utf8' })
                const file = buildUploadInputFrom({
                    stream, filename: `test_${dayjs().format('DD_MM')}.csv`, mimetype: 'text/csv', encoding: 'utf8',
                    meta: {},
                })

                console.log('filename', filename)
                
                return {
                    ok: true,
                }
            },
        },
    ],
    
})

module.exports = {
    _internalSendResidentPhonesService,
}